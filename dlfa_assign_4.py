# -*- coding: utf-8 -*-
"""DLFA_ASSIGN_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cn7xcw6prhToVAPct-b_eCsqWiIfe7L2
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=False)

for batch in train_loader:
    inputs, labels = batch
    print("Shape of input data:", inputs.shape)
    print("Shape of labels:", labels.shape)
    break

figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()
    img, label = train_dataset[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()

examples = enumerate(test_loader)
batch_idx, (example_data, example_targets) = next(examples)

import matplotlib.pyplot as plt

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Ground Truth: {}".format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])
fig

"""# **CNN- Vanilla Implementation**"""

import torch
import torch.nn as nn

class CNNVanilla(nn.Module):
    def __init__(self):
        super(CNNVanilla, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 3 * 3, 512)  # Adjusted input size based on the output shape of the last conv layer
        self.fc2 = nn.Linear(512, 10)  # Adjusted for 10 output classes instead of 3

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = self.pool(nn.functional.relu(self.conv3(x)))
        x = x.view(-1, 64 * 3 * 3)  # Flatten based on the output shape of the last conv layer
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Instantiate the model
model_vanilla = CNNVanilla()

# Check the summary of the model
print(model_vanilla)

"""# **CNN-RESNET Implementation**"""

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += self.downsample(identity)
        out = self.relu(out)
        return out


class CNNResNet(nn.Module):
    def __init__(self, block, layers, num_classes=10):
        super(CNNResNet, self).__init__()
        self.in_channels = 16
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self.make_layer(block, 16, layers[0], stride=1)
        self.layer2 = self.make_layer(block, 32, layers[1], stride=2)
        self.layer3 = self.make_layer(block, 64, layers[2], stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, num_classes)

    def make_layer(self, block, out_channels, blocks, stride):
        strides = [stride] + [1] * (blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

"""# **Training Function**"""

def train(model, criterion, optimizer, train_loader, num_epochs=50):
    model.train()
    train_accuracy = []
    for epoch in range(num_epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)

            # Calculate accuracy
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_accuracy = correct / total
        train_accuracy.append(epoch_accuracy)

        print('[%d] loss: %.3f, accuracy: %.3f' %
              (epoch + 1, running_loss / len(train_loader.dataset), epoch_accuracy))
    print('Finished Training')
    return train_accuracy

def evaluate(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f"Accuracy on test set: {accuracy}%")

# 1. Train CNN-Vanilla and CNN-Resnet
cnn_vanilla = CNNVanilla()
cnn_resnet = CNNResNet(ResidualBlock, [2, 2, 2])  # ResNet with 3-level blocks

criterion = nn.CrossEntropyLoss()
optimizer_vanilla = optim.Adam(cnn_vanilla.parameters(), lr=0.001)
optimizer_resnet = optim.Adam(cnn_resnet.parameters(), lr=0.001)

train_accuracy_vanilla = train(cnn_vanilla, criterion, optimizer_vanilla, train_loader)
train_accuracy_resnet = train(cnn_resnet, criterion, optimizer_resnet, train_loader)

# Plot training accuracy vs epochs
epochs = range(1, 50 + 1)
plt.plot(epochs, train_accuracy_vanilla, label='CNN-Vanilla')
plt.plot(epochs, train_accuracy_resnet, label='CNN-ResNet')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Determine the best choice
best_choice = 'CNN-ResNet' if max(train_accuracy_resnet) > max(train_accuracy_vanilla) else 'CNN-Vanilla'
print(f"The best choice among CNN-Vanilla and CNN-ResNet is: {best_choice}")

"""#Experiment 1: Evaluate the performance of both models"""

print("Performance on test set:")
evaluate(cnn_vanilla,test_loader)
evaluate(cnn_resnet, test_loader)
num_params_vanilla = sum(p.numel() for p in cnn_vanilla.parameters())
print(f"Number of parameters (CNN-Vanilla): {num_params_vanilla}")
num_params_resnet = sum(p.numel() for p in cnn_resnet.parameters())
print(f"Number of parameters (CNN-Resnet): {num_params_resnet}")

"""#Experiment 2: Study the Effect of Data Normalization"""

transform_no_norm = transforms.Compose([
    transforms.ToTensor()
])

# Load datasets without normalization
train_dataset_no_norm = torchvision.datasets.MNIST(root='./data', train=True, transform=transform_no_norm, download=True)
test_dataset_no_norm = torchvision.datasets.MNIST(root='./data', train=False, transform=transform_no_norm, download=True)

# Create data loaders
train_loader_no_norm = DataLoader(dataset=train_dataset_no_norm, batch_size=256, shuffle=True)
test_loader_no_norm = DataLoader(dataset=test_dataset_no_norm, batch_size=256, shuffle=False)

# Train the best network without normalization
cnn_resnet_no_norm = CNNResNet(ResidualBlock, [2, 2, 2])
optimizer_resnet_no_norm = optim.Adam(cnn_resnet_no_norm.parameters(), lr=0.001)
train_resnet_without_norm=train(cnn_resnet_no_norm, criterion, optimizer_resnet_no_norm, train_loader_no_norm)

# Evaluate the performance of both models
print("Performance on test set with normalization:")
evaluate(cnn_resnet, test_loader)
print("Performance on test set without normalization:")
evaluate(cnn_resnet_no_norm, test_loader_no_norm)

# Plot training accuracy vs epochs for both cases of normalization
epochs = range(1, 50 + 1)
plt.plot(epochs, train_accuracy_resnet, label='With Normalization')
plt.plot(epochs, train_resnet_without_norm, label='No Normalization')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs Epochs (With vs No Normalization)')
plt.legend()
plt.grid(True)
plt.show()

# Determine the best choice
best_choice_norm = 'With Normalization' if max(train_accuracy_resnet) > max(train_resnet_without_norm) else 'No Normalization'
print(f"The best choice among with and without normalization is: {best_choice_norm}")

"""# Experiment 3: Study the Effect of Different Optimizers"""

optimizers = [
    optim.SGD(cnn_resnet.parameters(), lr=0.001),
    optim.SGD(cnn_resnet.parameters(), lr=0.001, momentum=0.9),
    optim.Adam(cnn_resnet.parameters(), lr=0.001)
]
optimizer_names = ['SGD', 'SGD with Momentum', 'Adam']
train_accuracies_all = []
for opt in optimizers:
    print(f"Training with optimizer: {opt}")
    train(cnn_resnet, criterion, opt, train_loader)
    print("Performance on test set:")
    evaluate(cnn_resnet, test_loader)


for opt, opt_name in zip(optimizers, optimizer_names):
    print(f"Training with optimizer: {opt_name}")
    train_accuracies = train(cnn_resnet, criterion, opt, train_loader)
    train_accuracies_all.append(train_accuracies)
    print("Performance on test set:")
    evaluate(cnn_resnet, test_loader)

# Plot training accuracy vs epochs for different optimizers
epochs = range(1,50 + 1)
for train_accuracies, opt_name in zip(train_accuracies_all, optimizer_names):
    plt.plot(epochs, train_accuracies, label=opt_name)

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs Epochs (Different Optimizers)')
plt.legend()
plt.grid(True)
plt.show()

"""#Experiment 4: Study the Effect of Network Depth"""

class CNNResNetFour(nn.Module):
    def __init__(self, block, layers, num_classes=10):
        super(CNNResNetFour, self).__init__()
        self.in_channels = 16
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self.make_layer(block, 16, layers[0], stride=1)
        self.layer2 = self.make_layer(block, 32, layers[1], stride=2)
        self.layer3 = self.make_layer(block, 64, layers[2], stride=2)
        self.layer4 = self.make_layer(block, 128, layers[3], stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(128, num_classes)

    def make_layer(self, block, out_channels, blocks, stride):
        strides = [stride] + [1] * (blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
# Train the deeper ResNet
cnn_resnet_four = CNNResNetFour(ResidualBlock, [2, 2, 2, 2])
optimizer_resnet_four = optim.Adam(cnn_resnet_four.parameters(), lr=0.001)
train(cnn_resnet_four, criterion, optimizer_resnet_four, train_loader)

# Compare performances
print("Performance on test set (Original ResNet):")
evaluate(cnn_resnet, test_loader)
print("Performance on test set (Deeper ResNet):")
evaluate(cnn_resnet_four, test_loader)

# Compare number of parameters
num_params_resnet_four = sum(p.numel() for p in cnn_resnet_four.parameters())
print(f"Number of parameters (Original ResNet): {num_params_resnet}")
print(f"Number of parameters (Deeper ResNet): {num_params_resnet_four}")

"""#Experiment 5: Study the Effect of Different Regularizers"""

class CNNResNetWithRegularizers(nn.Module):
    def __init__(self, block, layers, num_classes=10):
        super(CNNResNetWithRegularizers, self).__init__()
        self.in_channels = 16
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self.make_layer(block, 16, layers[0], stride=1)
        self.layer2 = self.make_layer(block, 32, layers[1], stride=2)
        self.layer3 = self.make_layer(block, 64, layers[2], stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, num_classes)  # Adjusted based on the output shape of avg_pool

        # Define regularizers
        self.batch_norm = nn.BatchNorm1d(num_classes)  # BatchNorm1d for fully connected layer
        self.dropout = nn.Dropout(p=0.5)

    def make_layer(self, block, out_channels, blocks, stride):
        strides = [stride] + [1] * (blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        # Apply regularizers
        x = self.batch_norm(x)
        x = self.dropout(x)
        return x


# Train with regularizers
cnn_resnet_with_reg = CNNResNetWithRegularizers(ResidualBlock, [2, 2, 2])
optimizer_resnet_with_reg = optim.Adam(cnn_resnet_with_reg.parameters(), lr=0.001)
train(cnn_resnet_with_reg, criterion, optimizer_resnet_with_reg, train_loader)

# Evaluate the performance
print("Performance on test set with regularizers:")
evaluate(cnn_resnet_with_reg, test_loader)

# Check if GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU is not available, running on CPU")

import matplotlib.pyplot as plt

# Training accuracy data
epochs = list(range(1, 51))
cnn_vanilla_acc = [0.889, 0.977, 0.985, 0.988, 0.990, 0.992, 0.993, 0.994, 0.995, 0.996,
                   0.997, 0.996, 0.997, 0.997, 0.998, 0.997, 0.998, 0.998, 0.998, 0.998,
                   0.998, 0.998, 0.998, 0.999, 0.999, 0.999, 0.999, 0.998, 0.999, 0.999,
                   0.999, 0.999, 0.999, 1.000, 0.999, 0.998, 0.998, 1.000, 0.999, 0.999,
                   0.999, 0.999, 0.999, 1.000, 0.999, 0.999, 0.999, 1.000, 1.000, 1.000]

cnn_resnet_acc = [0.939, 0.986, 0.991, 0.992, 0.994, 0.995, 0.995, 0.995, 0.996, 0.997,
                  0.997, 0.998, 0.998, 0.997, 0.998, 0.998, 0.998, 0.998, 0.998, 0.998,
                  0.999, 0.999, 0.999, 0.998, 0.999, 0.998, 0.999, 0.999, 0.999, 0.999,
                  1.000, 0.998, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 1.000,
                  1.000, 1.000, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 1.000]

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(epochs, cnn_vanilla_acc, label='CNN-Vanilla')
plt.plot(epochs, cnn_resnet_acc, label='CNN-ResNet')
plt.title('Training Accuracy vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Dummy data representing training accuracies for each optimizer
train_accuracies_sgd = [0.88, 0.91, 0.93, 0.94, 0.95, 0.955, 0.958, 0.96, 0.963, 0.965,
                        0.967, 0.968, 0.969, 0.97, 0.971, 0.972, 0.973, 0.974, 0.975, 0.976,
                        0.977, 0.978, 0.979, 0.98, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986,
                        0.987, 0.988, 0.989, 0.99, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996,
                        0.997, 0.998, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

train_accuracies_sgd_momentum = [0.88, 0.93, 0.95, 0.96, 0.965, 0.968, 0.97, 0.972, 0.974, 0.975,
                                  0.976, 0.978, 0.979, 0.98, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986,
                                  0.987, 0.988, 0.989, 0.99, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996,
                                  0.997, 0.998, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                                  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

train_accuracies_adam = [0.88, 0.91, 0.94, 0.95, 0.96, 0.965, 0.968, 0.97, 0.972, 0.974, 0.975,
                         0.976, 0.978, 0.979, 0.98, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986,
                         0.987, 0.988, 0.989, 0.99, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996,
                         0.997, 0.998, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                         1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

# Plotting
epochs = range(1, 51)  # Assuming 50 epochs
plt.plot(epochs, train_accuracies_sgd, label='SGD')
plt.plot(epochs, train_accuracies_sgd_momentum, label='SGD with Momentum')
plt.plot(epochs, train_accuracies_adam, label='Adam')

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs Epochs (Different Optimizers)')
plt.legend()
plt.grid(True)
plt.show()